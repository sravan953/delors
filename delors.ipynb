{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Keerthi Sravan Ravi\n",
    "# date: 21/12/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as kb\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, BatchNormalization, MaxPool2D\n",
    "from keras.optimizers import adam\n",
    "from keras.initializers import Constant, glorot_uniform\n",
    "from keras.utils import plot_model\n",
    "import sklearn.model_selection\n",
    "import scipy.misc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and saving methods\n",
    "---\n",
    "\n",
    "#### 1. load_data_from_disk():\n",
    "- Load spiral k-space trajectories (x data) from folder `imagenet_kspace`\n",
    "- Load reconstructed images (y data) from folder `imagenet_orig`\n",
    "\n",
    "#### 2. save_data_as_npy(arr_path_dict)\n",
    "For faster access to data, we save the data as `.npy` files.\n",
    "\n",
    "#### 3. load_data_from_npy(*paths)\n",
    "In case we have already saved data as `.npy` files, we load the same, as opposed to loading data from `load_data()`.\n",
    "\n",
    "#### 4. gen_train_test_data(x, y)\n",
    "Split data into training and testing sets, of proportion:\n",
    "- Train: 95%\n",
    "- Test: 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_disk():\n",
    "    \"\"\"\n",
    "    Load x and y data from 'imagenet_64_orig' and 'imagenet_64_kspace'.\n",
    "    Typically, these two folders should be in the same folder as this code.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_path = os.getcwd()\n",
    "    # Find data from 'imagenet_64_orig' and 'imagenet_64_kspace'\n",
    "    src_imagenet_orig = os.path.join(root_path, 'imagenet_64_orig')\n",
    "    src_imagenet_kspace = os.path.join(root_path, 'imagenet_64_kspace')\n",
    "\n",
    "    all_folders = ['animals','artefact','flora','fungus','geo','people', 'sports1']\n",
    "    print('Folders to load from: {}'.format(all_folders))\n",
    "    x, y = [], []\n",
    "\n",
    "    for curr_folder in all_folders:\n",
    "        print('Currently in folder \\'{}\\''.format(curr_folder))\n",
    "        img_folder = os.path.join(src_imagenet_orig, curr_folder)\n",
    "        kspace_folder = os.path.join(src_imagenet_kspace, curr_folder)\n",
    "\n",
    "        num_files = len(os.listdir(kspace_folder))\n",
    "        for i in range(1, num_files):\n",
    "            img_file = str(i) + '.jpg'\n",
    "            kspace_file = str(i) + '.npy'\n",
    "\n",
    "            img_path = os.path.join(img_folder, img_file)\n",
    "            kspace_path = os.path.join(kspace_folder, kspace_file)\n",
    "\n",
    "            img = scipy.misc.imread(img_path)\n",
    "            img_size = img.size\n",
    "            \n",
    "            # Skip loading null image\n",
    "            # If more than 90% of the image is white pixels, then it is a null image\n",
    "            null_pixels = len(np.where(img >= 255)[0])\n",
    "            if null_pixels >= 0.9 * img_size:\n",
    "                continue        \n",
    "\n",
    "            kspace = np.load(kspace_path)\n",
    "            # Normalize k-space data\n",
    "            kspace = 127 * kspace/np.amax(np.abs(kspace))\n",
    "            kspace_real = np.real(kspace)\n",
    "            kspace_imag = np.imag(kspace)\n",
    "            # Convert to int16 to save memory\n",
    "            kspace = np.stack((kspace_real, kspace_imag), axis=2).astype('int16')\n",
    "\n",
    "            x.append(kspace)\n",
    "            y.append(img)\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "def save_data_as_npy(arr_path_dict):\n",
    "    \"\"\"\n",
    "    Save data to .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    arr_path_dict : dict\n",
    "        Key-value pairs of the array to be saved, and the save paths\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in arr_path_dict.keys():\n",
    "        np.save(arr_path_dict[item], item)\n",
    "        \n",
    "def load_data_from_npy(*paths):\n",
    "    \"\"\"\n",
    "    Load data from .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    paths : list\n",
    "        List of paths to load .npy arrays from.\n",
    "\n",
    "    Returns:\n",
    "    An array of the .npy files that were loaded from the specified paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    arrs = []\n",
    "    for _path in paths:\n",
    "        print('Loading {}'.format(_path))\n",
    "        arrs.append(np.load(_path))\n",
    "    \n",
    "    print('Done.')\n",
    "    return arrs\n",
    "\n",
    "def gen_train_test_data(x, y):\n",
    "    \"\"\"\n",
    "    Generate 95% training and 5% testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    x : ndarray\n",
    "        X data\n",
    "    y : ndarray\n",
    "        Y data\n",
    "        \n",
    "    Returns:\n",
    "    A tuple of x_train, x_test, y_train and y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    x_tr, x_ts, y_tr, y_ts = sklearn.model_selection.train_test_split(np.array(x), np.array(y), train_size=0.95)\n",
    "\n",
    "    print('Training dataset contains {} images'.format(len(x_tr)))\n",
    "    print('Testing dataset contains {} images'.format(len(y_ts)))\n",
    "    \n",
    "    return x_tr, x_ts, y_tr, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback for normalizing losses\n",
    "class NormalizeLosses_Callback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.max_loss = 0\n",
    "        self.all_losses = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # We want to record the maximum loss across epochs in this run to normalize losses    \n",
    "        loss = logs.get('loss')\n",
    "        self.max_loss = loss if loss > self.max_loss else self.max_loss\n",
    "        self.all_losses = self.all_losses.append(loss / self.max_loss)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, save and load Keras models\n",
    "---\n",
    "\n",
    "#### 1. get_keras_model(return_modified_model=False)\n",
    "Returns the original Keras Sequential model.\n",
    "\n",
    "#### 2. save_keras_model_to_disk(model, path)\n",
    "Save Keras model to disk.\n",
    "\n",
    "#### 3. load_keras_model_from_disk(path)\n",
    "Load Keras model from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_model(return_modified_model=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    return_modified_model : boolean\n",
    "        Boolean flag to indicate whether the model returned is original or modified.\n",
    "        \n",
    "    Returns:\n",
    "    Keras Sequential model with the following layers, in order:\n",
    "    - Dense (64 * 64) X 3\n",
    "    - Reshape\n",
    "    - { Conv2D(5, 64)\n",
    "    - BatchNormalization } X 3 (4 if return_modified_model is True)\n",
    "    - Conv2DTranspose(5, 64)\n",
    "    - BatchNormalization\n",
    "    - Flatten\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clear current Keras session, and create a Sequential model\n",
    "    kb.clear_session()\n",
    "    model = Sequential()\n",
    "\n",
    "    # Constant bias = 0.05, and\n",
    "    # Truncated normal filter with std = 0.05\n",
    "    bias_initializer = Constant(value=0.1)\n",
    "    kernel = glorot_uniform()\n",
    "\n",
    "    size = 64 * 64\n",
    "    \n",
    "    # Fully connected layers 1, 2 and 3\n",
    "    model.add(Dense(size, use_bias=True, activation='relu', bias_initializer=bias_initializer,\n",
    "                                 input_shape=(32480,)))\n",
    "    model.add(Dense(size, use_bias=True, activation='relu', bias_initializer=bias_initializer))\n",
    "    model.add(Dense(size, use_bias=True, activation='relu', bias_initializer=bias_initializer))\n",
    "\n",
    "    # Reshape the outputs so far to prepape for convolutional processing\n",
    "    model.add(keras.layers.Reshape((64, 64, 1)))\n",
    "\n",
    "    # Convolution layers 1, 2 and 3 (and 4 if return_modified_model=True)\n",
    "    filter_size = 5\n",
    "    num_filters = 64\n",
    "    model.add(Conv2D(num_filters, filter_size, kernel_initializer=kernel, activation='relu',\n",
    "                            use_bias=True, bias_initializer=bias_initializer, padding='same'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Conv2D(num_filters, filter_size, kernel_initializer=kernel, activation='relu',\n",
    "                            use_bias=True, bias_initializer=bias_initializer, padding='same'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Conv2D(num_filters, filter_size, kernel_initializer=kernel, activation='relu',\n",
    "                            use_bias=True, bias_initializer=bias_initializer, padding='same'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    if return_modified_model:\n",
    "        model.add(Conv2D(num_filters, filter_size, kernel_initializer=kernel, activation='relu',\n",
    "                                use_bias=True, bias_initializer=bias_initializer, padding='same'))\n",
    "        model.add(BatchNormalization(center=True, scale=True))\n",
    "\n",
    "    # Deconvolution layer 1\n",
    "    num_deconv_filters = 1\n",
    "    model.add(Conv2DTranspose(num_deconv_filters, filter_size, kernel_initializer=kernel, activation='relu',\n",
    "                        use_bias=True, bias_initializer=bias_initializer, padding='same'))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "\n",
    "    # Flatten the outputs\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Define optimizer, instantiate loss-callback, and compile\n",
    "    model.compile(optimizer=adam(lr=5e-3), metrics=['acc'], loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_keras_model_to_disk(model, path):\n",
    "    \"\"\"\n",
    "    Save Keras model to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    model : Sequential\n",
    "        Keras Sequential model to save.\n",
    "    path : str\n",
    "        Path to save Keras model to.\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(path, 'keras_model.h5')\n",
    "    model.save(save_path)\n",
    "    \n",
    "def load_keras_model_from_disk(path):\n",
    "    \"\"\"\n",
    "    Load Keras model from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    path : str\n",
    "        Path to load Keras model from.\n",
    "    \"\"\"\n",
    "    return load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions\n",
    "---\n",
    "Run the network on, say, 12 k-space data samples (from `x_testing`). Plot the reconstructions versus the ground truth (from `y_testing`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_truth(model, x_ts, y_ts):\n",
    "    \"\"\"\n",
    "    Plot 12 predictions (x_testing) and ground truths (y_testing).\n",
    "    \n",
    "    Parameters:\n",
    "    model : Sequential\n",
    "        A Keras Sequential model used for predictions.\n",
    "    x_ts : ndarray\n",
    "        X testing data \n",
    "    y_ts : ndarray\n",
    "        Y testing data\n",
    "    num_predictions : int\n",
    "        Number of predictions to generate and plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # K-space samples are 280 samples-58 shot, complex numbers\n",
    "    kspace_size = 280 * 58 * 2\n",
    "    # Number of predictions\n",
    "    num_predictions = 12\n",
    "    # Random index of sample to predict\n",
    "    starting_index = np.random.randint(len(x_ts) - num_predictions)\n",
    "    plot_index = 1\n",
    "    plt.figure(num=1, figsize=(14, 6))\n",
    "    plt.suptitle('Reconstructions', fontsize=16)\n",
    "    for x in x_ts[starting_index : starting_index + num_predictions]:\n",
    "        x = x.reshape(-1, kspace_size)\n",
    "        y_pred = model.predict(x)\n",
    "        y_pred = y_pred.reshape((64, 64))\n",
    "        # Plot 2 rows, 6 columns\n",
    "        plt.subplot(2, 6, plot_index)\n",
    "        plt.imshow(y_pred)\n",
    "        plot_index+=1\n",
    "\n",
    "    img_flat_shape = (64, 64)\n",
    "    plot_index = 1\n",
    "    plt.figure(num=2, figsize=(14, 6))\n",
    "    plt.suptitle('Ground truths', fontsize=1)\n",
    "    for y in y_ts[starting_index : starting_index + num_predictions]:\n",
    "        y = y.reshape(img_flat_shape)\n",
    "        # Plot 2 rows, 6 columns\n",
    "        plt.subplot(2, 6, plot_index)\n",
    "        plt.imshow(y)\n",
    "        plot_index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the network\n",
    "---\n",
    "Now, run everything that we've made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and generate training and testing sets\n",
    "x, y = load_data_from_npy('x.npy', 'y.npy')\n",
    "x_tr, x_ts, y_tr, y_ts = gen_train_test_data(x, y)\n",
    "\n",
    "# Intantiate a normalized-loss callback, and get a Keras model\n",
    "loss_callback = NormalizeLosses_Callback()\n",
    "model = get_keras_model()\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model for 50 epochs, passing the normalzied-loss callback we just created\n",
    "model.fit(x_tr, y_tr, batch_size=32, epochs=1, validation_data=(x_ts, y_ts), callbacks=[loss_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 12 predictions and their ground truths\n",
    "plot_predictions_vs_truth(model, x_ts, y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
